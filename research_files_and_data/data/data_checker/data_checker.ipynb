{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg2RtWJ6z4bz",
        "outputId": "323c4628-8a05-4d17-b2f3-c73825a720aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 Loaded 10 files → 15,232 rows, 30 columns\n",
            "\n",
            "🧾 Tickers loaded: AAPL, AMD, AMZN, CRM, GOOGL, INTC, META, MSFT, NVDA, TSLA \n",
            "        rows       start         end  days_span\n",
            "Ticker                                         \n",
            "META    1525  2019-01-03  2025-10-02       2464\n",
            "AAPL    1523  2019-01-04  2025-10-02       2463\n",
            "AMD     1523  2019-01-04  2025-10-02       2463\n",
            "AMZN    1523  2019-01-04  2025-10-02       2463\n",
            "GOOGL   1523  2019-01-04  2025-10-02       2463\n",
            "CRM     1523  2019-01-04  2025-10-02       2463\n",
            "INTC    1523  2019-01-04  2025-10-02       2463\n",
            "MSFT    1523  2019-01-04  2025-10-02       2463\n",
            "NVDA    1523  2019-01-04  2025-10-02       2463\n",
            "TSLA    1523  2019-01-04  2025-10-02       2463\n",
            "\n",
            "🔎 Duplicates (Ticker+Date): 0\n",
            "\n",
            "🕳️  Missingness by column (top 15):\n",
            "Sent_5d_Avg       3.17%\n",
            "Ticker             0.0%\n",
            "Volume             0.0%\n",
            "SMA_5              0.0%\n",
            "EMA_12             0.0%\n",
            "Date               0.0%\n",
            "RSI_14             0.0%\n",
            "MACD               0.0%\n",
            "BB_upper           0.0%\n",
            "BB_lower           0.0%\n",
            "Momentum_10        0.0%\n",
            "Volatility_10d     0.0%\n",
            "SMA_5_XLK          0.0%\n",
            "EMA_26             0.0%\n",
            "EMA_12_XLK         0.0%\n",
            "dtype: object\n",
            "\n",
            "📰 Detected sentiment-related columns: DailySent, HasSent, Sent_5d_Avg, SentimentSignal, bullish_sentiment_rsi, bearish_sentiment_rsi\n",
            "\n",
            "🎯 Target 'y_5d' distribution (overall):\n",
            "  class 1: 7759 rows (50.94%)\n",
            "  class 0: 7473 rows (49.06%)\n",
            "\n",
            "🎯 Target balance per ticker (head):\n",
            "        p_class_0  p_class_1  count\n",
            "Ticker                             \n",
            "AAPL     0.487853   0.512147   1523\n",
            "AMD      0.484570   0.515430   1523\n",
            "AMZN     0.502298   0.497702   1523\n",
            "CRM      0.482600   0.517400   1523\n",
            "GOOGL    0.487853   0.512147   1523\n",
            "INTC     0.506894   0.493106   1523\n",
            "META     0.493770   0.506230   1525\n",
            "MSFT     0.479317   0.520683   1523\n",
            "NVDA     0.478661   0.521339   1523\n",
            "TSLA     0.502298   0.497702   1523\n",
            "\n",
            "🧮 Numeric feature columns: 25\n",
            "Volume, SMA_5, EMA_12, EMA_26, RSI_14, MACD, BB_upper, BB_lower, Momentum_10, Volatility_10d, SMA_5_XLK, EMA_12_XLK, EMA_26_XLK, RSI_14_XLK, MACD_XLK, BB_upper_XLK, BB_lower_XLK, Momentum_10_XLK, Volatility_10d_XLK, DailySent, HasSent, Sent_5d_Avg, SentimentSignal, bullish_sentiment_rsi, bearish_sentiment_rsi\n",
            "\n",
            "📈 Top 10 |corr(feature, y_5d)|:\n",
            "MACD                 -0.1148\n",
            "Volatility_10d_XLK    0.1068\n",
            "BB_lower_XLK         -0.1040\n",
            "SMA_5_XLK            -0.1038\n",
            "EMA_12_XLK           -0.1023\n",
            "EMA_26_XLK           -0.0987\n",
            "BB_upper_XLK         -0.0950\n",
            "SMA_5                -0.0881\n",
            "EMA_12               -0.0861\n",
            "MACD_XLK             -0.0849\n",
            "\n",
            "🪄 Common overlap window: 2019-01-04 → 2025-10-02 (2463 days)\n",
            "\n",
            "💾 Saved CSVs to: /Users/nikita/Documents/final_project_data\n",
            "\n",
            "Summary: {'n_files': 10, 'n_rows': 15232, 'n_cols': 30, 'tickers': ['AAPL', 'AMD', 'AMZN', 'CRM', 'GOOGL', 'INTC', 'META', 'MSFT', 'NVDA', 'TSLA'], 'out_dir': '/Users/nikita/Documents/final_project_data', 'target_present': True, 'n_features_numeric': 25}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _ensure_dir(p: Path | str) -> Path:\n",
        "    p = Path(p)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "def _read_csv_smart(path: Path, ticker_from_name: Optional[str]) -> pd.DataFrame:\n",
        "    \"\"\"Read a CSV and normalize basic columns.\"\"\"\n",
        "    df = pd.read_csv(path, low_memory=False)\n",
        "    if \"Ticker\" not in df.columns and ticker_from_name:\n",
        "        df.insert(0, \"Ticker\", ticker_from_name.upper())\n",
        "\n",
        "    # Normalize Date\n",
        "    if \"Date\" in df.columns:\n",
        "        parsed = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "        df[\"Date\"] = parsed.dt.strftime(\"%Y-%m-%d\").where(parsed.notna(), df[\"Date\"].astype(str).str[:10])\n",
        "\n",
        "    # Standardize Ticker casing\n",
        "    if \"Ticker\" in df.columns:\n",
        "        df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def analyze_final_data(data_dir: str, out_dir: Optional[str] = None, save_csv: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Load all per‑ticker CSVs from a folder, print useful diagnostics,\n",
        "    and optionally save summary CSVs. Returns a small dict of headline stats.\n",
        "    \"\"\"\n",
        "    p = Path(data_dir).expanduser().resolve()\n",
        "    if not p.exists() or not p.is_dir():\n",
        "        raise FileNotFoundError(f\"Folder not found: {p}\")\n",
        "\n",
        "    files = sorted(p.glob(\"*.csv\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {p}\")\n",
        "\n",
        "    dfs: List[pd.DataFrame] = []\n",
        "    for f in files:\n",
        "        ticker = f.stem.split(\".\")[0].upper()\n",
        "        try:\n",
        "            df = _read_csv_smart(f, ticker)\n",
        "            df[\"__sourcefile\"] = f.name\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Failed to read {f.name}: {e}\")\n",
        "\n",
        "    if not dfs:\n",
        "        raise RuntimeError(\"No readable CSVs.\")\n",
        "\n",
        "    full = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"\\n📦 Loaded {len(files)} files → {len(full):,} rows, {len(full.columns)} columns\")\n",
        "    if \"Ticker\" not in full.columns:\n",
        "        raise RuntimeError(\"No 'Ticker' column found after loading.\")\n",
        "\n",
        "    # Basic expectations\n",
        "    target_col = \"y_5d\" if \"y_5d\" in full.columns else None\n",
        "    price_cols = [c for c in [\"Adj Close\", \"Close\", \"Open\", \"High\", \"Low\"] if c in full.columns]\n",
        "\n",
        "    # Per-ticker stats\n",
        "    grp = full.groupby(\"Ticker\", dropna=False, sort=True)\n",
        "    per_ticker_rows = grp.size().rename(\"rows\")\n",
        "    per_ticker_start = grp[\"Date\"].min() if \"Date\" in full.columns else pd.Series(dtype=object)\n",
        "    per_ticker_end = grp[\"Date\"].max() if \"Date\" in full.columns else pd.Series(dtype=object)\n",
        "\n",
        "    per_ticker = pd.concat([per_ticker_rows, per_ticker_start.rename(\"start\"), per_ticker_end.rename(\"end\")], axis=1)\n",
        "    if \"Date\" in full.columns:\n",
        "        per_ticker[\"days_span\"] = (\n",
        "            pd.to_datetime(per_ticker[\"end\"]) - pd.to_datetime(per_ticker[\"start\"])\n",
        "        ).dt.days\n",
        "\n",
        "    print(\"\\n🧾 Tickers loaded:\", \", \".join(per_ticker.index.tolist()[:20]),\n",
        "          (\"...\" if len(per_ticker) > 20 else \"\"))\n",
        "    print(per_ticker.sort_values(\"rows\", ascending=False).head(10).to_string())\n",
        "\n",
        "    # Duplicates by (Ticker, Date)\n",
        "    if \"Date\" in full.columns:\n",
        "        dup_mask = full.duplicated(subset=[\"Ticker\", \"Date\"], keep=False)\n",
        "        dup_cnt = int(dup_mask.sum())\n",
        "        print(f\"\\n🔎 Duplicates (Ticker+Date): {dup_cnt}\")\n",
        "        if dup_cnt > 0:\n",
        "            print(full.loc[dup_mask, [\"Ticker\", \"Date\", \"__sourcefile\"]].head(10).to_string())\n",
        "\n",
        "    # Missingness\n",
        "    miss = full.isna().mean().sort_values(ascending=False)\n",
        "    print(\"\\n🕳️  Missingness by column (top 15):\")\n",
        "    print((miss.head(15) * 100).round(2).astype(str) + \"%\")\n",
        "\n",
        "    # Optional sentiment presence\n",
        "    cand_sent = [c for c in full.columns if \"sent\" in c.lower()]\n",
        "    if cand_sent:\n",
        "        print(\"\\n📰 Detected sentiment-related columns:\", \", \".join(cand_sent[:30]))\n",
        "\n",
        "    # Target distribution\n",
        "    if target_col is not None:\n",
        "        vc = full[target_col].value_counts(dropna=False)\n",
        "        vcp = (vc / vc.sum() * 100).round(2)\n",
        "        print(f\"\\n🎯 Target '{target_col}' distribution (overall):\")\n",
        "        for k, v in vc.items():\n",
        "            print(f\"  class {k}: {v} rows ({vcp[k]}%)\")\n",
        "\n",
        "        per_t_bal = (\n",
        "            full.groupby(\"Ticker\")[target_col]\n",
        "            .value_counts(normalize=True)\n",
        "            .unstack(fill_value=0.0)\n",
        "            .rename(columns=lambda c: f\"p_class_{c}\")\n",
        "        )\n",
        "        per_t_cnt = full.groupby(\"Ticker\")[target_col].size().rename(\"count\")\n",
        "        bal = per_t_bal.join(per_t_cnt)\n",
        "        print(\"\\n🎯 Target balance per ticker (head):\")\n",
        "        print(bal.head(10).to_string())\n",
        "\n",
        "        # Flag severe imbalance\n",
        "        if {\"p_class_0\", \"p_class_1\"}.issubset(bal.columns):\n",
        "            severe = bal[(bal[\"p_class_0\"] < 0.1) | (bal[\"p_class_1\"] < 0.1)]\n",
        "            if len(severe) > 0:\n",
        "                print(\"\\n⚠️  Severely imbalanced tickers (one class < 10%):\")\n",
        "                print(severe.sort_values(\"count\", ascending=False).head(20).to_string())\n",
        "\n",
        "    # Feature space\n",
        "    numeric_dtypes = (\"int16\",\"int32\",\"int64\",\"float16\",\"float32\",\"float64\")\n",
        "    drop_cols = {\"Date\", \"__sourcefile\"}\n",
        "    if target_col: drop_cols.add(target_col)\n",
        "    drop_cols.update({\"Return_5d\"})  # no 'fwd_ret_5d' here per user's data\n",
        "    feature_cols = [c for c in full.columns if c not in drop_cols and str(full[c].dtype) in numeric_dtypes]\n",
        "    print(f\"\\n🧮 Numeric feature columns: {len(feature_cols)}\")\n",
        "    print(\", \".join(feature_cols[:25]) + (\" ...\" if len(feature_cols) > 25 else \"\"))\n",
        "\n",
        "    # Correlation with target (point-biserial via Pearson on 0/1)\n",
        "    if target_col and feature_cols:\n",
        "        y = full[target_col].astype(float)\n",
        "        corr = {}\n",
        "        for c in feature_cols:\n",
        "            try:\n",
        "                corr[c] = float(pd.Series(full[c]).astype(float).corr(y))\n",
        "            except Exception:\n",
        "                corr[c] = np.nan\n",
        "        corr_s = pd.Series(corr).dropna().sort_values(key=lambda x: x.abs(), ascending=False)\n",
        "        print(\"\\n📈 Top 10 |corr(feature, y_5d)|:\")\n",
        "        print(corr_s.head(10).round(4).to_string())\n",
        "\n",
        "    # Suspicious / leakage-prone names (generic)\n",
        "    sus_substrings = [\"fwd\", \"future\", \"lead\", \"ahead\", \"t+1\", \"t+2\", \"target\"]\n",
        "    sus = [c for c in full.columns if any(s in c.lower() for s in sus_substrings)]\n",
        "    if sus:\n",
        "        print(\"\\n🛑 Potential leakage columns (by name):\", \", \".join(sus))\n",
        "\n",
        "    # Monotonic date check per ticker\n",
        "    if \"Date\" in full.columns:\n",
        "        issues = []\n",
        "        for t, g in full.groupby(\"Ticker\"):\n",
        "            dt = pd.to_datetime(g[\"Date\"], errors=\"coerce\")\n",
        "            if dt.is_monotonic_increasing is False:\n",
        "                issues.append(t)\n",
        "        if issues:\n",
        "            print(\"\\n⚠️  Non‑monotonic Date order in tickers:\", \", \".join(issues[:20]),\n",
        "                  (\"...\" if len(issues) > 20 else \"\"))\n",
        "\n",
        "        # Common overlap window\n",
        "        span = grp[\"Date\"].agg([\"min\", \"max\"])\n",
        "        common_start = pd.to_datetime(span[\"min\"]).max()\n",
        "        common_end = pd.to_datetime(span[\"max\"]).min()\n",
        "        if pd.notna(common_start) and pd.notna(common_end) and common_start <= common_end:\n",
        "            days = (common_end - common_start).days\n",
        "            print(f\"\\n🪄 Common overlap window: {common_start.date()} → {common_end.date()} ({days} days)\")\n",
        "        else:\n",
        "            print(\"\\nℹ️  No common overlap window across all tickers.\")\n",
        "\n",
        "    # Save CSV outputs\n",
        "    odir = None\n",
        "    if save_csv:\n",
        "        odir = _ensure_dir(out_dir or (p / \"_analysis\"))\n",
        "        per_ticker.to_csv(odir / \"per_ticker_stats.csv\")\n",
        "        miss.rename(\"missing_rate\").to_frame().to_csv(odir / \"missing_by_column.csv\")\n",
        "        if target_col is not None:\n",
        "            bal.to_csv(odir / \"class_balance_per_ticker.csv\")\n",
        "        if target_col and feature_cols:\n",
        "            corr_s.rename(\"corr_with_y\").to_frame().to_csv(odir / \"feature_correlations.csv\")\n",
        "        print(f\"\\n💾 Saved CSVs to: {odir}\")\n",
        "\n",
        "    # Return a compact stats dict\n",
        "    return {\n",
        "        \"n_files\": len(files),\n",
        "        \"n_rows\": int(len(full)),\n",
        "        \"n_cols\": int(len(full.columns)),\n",
        "        \"tickers\": sorted(full[\"Ticker\"].unique().tolist()),\n",
        "        \"out_dir\": str(odir) if odir else None,\n",
        "        \"target_present\": bool(target_col is not None),\n",
        "        \"n_features_numeric\": len(feature_cols),\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # <<< EDIT THESE >>>\n",
        "    DATA_DIR = \"/content/drive/MyDrive/train_data\"\n",
        "    OUT_DIR = \"/Users/nikita/Documents/final_project_data\"       # or set to a path, e.g. \"/Users/nikita/Documents/stock_project_train_data/report\"\n",
        "    SAVE_CSV = True      # set False to only print and not save\n",
        "\n",
        "    stats = analyze_final_data(DATA_DIR, out_dir=OUT_DIR, save_csv=SAVE_CSV)\n",
        "    print(\"\\nSummary:\", stats)\n"
      ]
    }
  ]
}